# SMEAC Framework for Agentic AI Systems
## A Working Thesis on Military Planning Doctrine Applied to Autonomous Agent Architecture

**Project Status:** Active Development  
**Started:** February 14, 2026  
**Last Updated:** February 14, 2026  

---

## Executive Summary

This document explores the application of SMEAC (Situation, Mission, Execution, Administration & Logistics, Command & Signal)â€”a military planning and briefing frameworkâ€”to the design and operation of autonomous AI agent systems. Rather than treating SMEAC as a one-time briefing template, we propose it as a **recursive operational loop** and **architectural pattern** for multi-agent systems.

**Core Thesis:** Agentic AI systems suffer from common failure modes (context drift, premature optimization, lack of coordination) that military planning doctrine has already solved. By encoding SMEAC as the agent's "operating system," we can create more robust, auditable, and mission-focused autonomous systems.

---

## Table of Contents

1. [Background & Context](#background--context)
2. [The SMEAC Framework Explained](#the-smeac-framework-explained)
3. [Core Insight: SMEAC as Recursive Loop](#core-insight-smeac-as-recursive-loop)
4. [Architectural Proposal](#architectural-proposal)
5. [Multi-Agent SMEAC Design](#multi-agent-smeac-design)
6. [Memory Architecture](#memory-architecture)
7. [Safety as First-Class Concern](#safety-as-first-class-concern)
8. [Mission Profiles & Doctrine Templates](#mission-profiles--doctrine-templates)
9. [Implementation Roadmap](#implementation-roadmap)
10. [Open Questions & Research Areas](#open-questions--research-areas)
11. [Practical Applications](#practical-applications)
12. [References & Prior Art](#references--prior-art)

---

## Background & Context

### Origin Story
This framework emerged from a conversation exploring SMEAC (initially encountered in a movie reference) and its potential application to OpenClaw agentic personality design. The core realization: **what works for military operations should work for autonomous agents**.

### Problem Statement
Current agentic AI systems commonly exhibit:
- **Context drift**: Agents lose sight of original objectives over long tasks
- **Premature action**: Jumping to execution without proper situation assessment
- **Poor coordination**: Multi-agent systems lack clear command structure
- **No audit trail**: Difficult to understand why agents made specific decisions
- **Resource mismanagement**: Token limits, API costs, and time budgets poorly tracked
- **Safety gaps**: Risks identified only after problems occur

### Why SMEAC?
Military planning doctrine has spent decades refining how to:
- Assess complex, uncertain situations
- Define clear, measurable objectives
- Plan and adapt under constraints
- Coordinate multiple actors
- Maintain safety and governance

These are **exactly the problems** agentic systems face.

---

## The SMEAC Framework Explained

### Traditional SMEAC (Military Briefing Format)

**S - Situation**
- What is happening now?
- Where is it happening (location/area)?
- What caused it / background context?
- What has been affected or is at risk (people, assets, systems)?
- What resources are already committed and what is available?

**M - Mission**
- One clear sentence describing what you must achieve, by when, and why
- Example pattern: "Our mission is to [do what] at [where] by [time] in order to [purpose]"

**E - Execution**
- Concept of operations (overall plan/approach)
- Tasks: who does what, when, and where
- Timings, phases, key milestones
- Coordination instructions (routes, boundaries, actions on X, contingencies)

**A - Administration & Logistics**
- Personnel, transport, equipment, supplies
- Medical/first aid support and evacuation plan if relevant
- Accommodation, welfare, and any contractors or external support

**C - Command & Signal**
- Who is in charge; chain of command (primary, alternates)
- How you will communicate: channels, platforms, frequencies, call signs, key numbers
- Reporting requirements: what, to whom, and how often

**+S - Safety** (SMEACS variant)
- Key hazards and controls, PPE requirements, safe zones/escape routes
- Location of medical support and any weather or environmental risks

---

## Core Insight: SMEAC as Recursive Loop

### Traditional Use: One-Shot Brief
In military contexts, SMEAC is delivered once before an operation begins.

### Agentic Innovation: Continuous Loop
For autonomous agents, SMEAC becomes a **state machine** that runs continuously:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SITUATION  â”‚â”€â”€> Update environment state, context, uncertainties
â”‚   UPDATE    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MISSION   â”‚â”€â”€> Validate/refine goals, constraints, success criteria
â”‚   REFINE    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXECUTION  â”‚â”€â”€> Generate/revise plans, select next action
â”‚    PLAN     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ADMIN    â”‚â”€â”€> Check resources, budgets, access before action
â”‚    CHECK    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   COMMAND   â”‚â”€â”€> Log actions, outcomes; communicate to other agents
â”‚   SIGNAL    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€> Loop back to SITUATION UPDATE
```

**The loop continues until:**
- Mission success criteria met
- Abort conditions triggered
- Human intervention requested
- Resource constraints exceeded

### Mapping SMEAC to Agent Capabilities

| SMEAC Element | Agent Capabilities |
|---------------|-------------------|
| **Situation** | Perception, context ingestion, environment state modeling, tool discovery, memory retrieval |
| **Mission** | Goal formulation, constraint identification, success metrics definition, priority stack |
| **Execution** | Multi-step planning, tool selection, action sequencing, contingency branches |
| **Admin & Logistics** | Resource accounting (tokens, rate limits, data sources, human time), access control |
| **Command & Signal** | Orchestration layer, role assignment (multi-agent), communication protocols, logging |

---

## Architectural Proposal

### SMEAC as Agent Operating System

Rather than a prompt engineering trick, encode SMEAC as **structural constraints** in the agent architecture:

#### 1. Forced Decomposition
- Every new task triggers automatic SMEAC decomposition
- Agent cannot proceed to Execution without completing S and M
- Checkpoints enforce human validation at key transitions

#### 2. State Machine Enforcement
- Agent tracks which SMEAC phase it's in
- Phase transitions have explicit requirements
- "SMEAC checkpoints": periodic validation that Situation hasn't changed enough to invalidate Mission

#### 3. Separation of Concerns
- Different agents/modules handle different SMEAC elements
- Clear interfaces between components
- No agent can skip phases or work outside its mandate

---

## Multi-Agent SMEAC Design

### Proposal: 5-Agent Architecture

Instead of monolithic agent, decompose by SMEAC responsibility:

#### **1. Situation Agent** (Reconnaissance Specialist)
**Mandate:** Build and maintain situational awareness
**Responsibilities:**
- Gather context from all available sources
- Identify knowns, unknowns, and assumptions
- Flag changes in environment that affect Mission
- Research background and causal factors
- Map resources available vs. committed

**Constraints:**
- Never plans or executes
- Never interprets user intent (that's Mission Agent's job)
- Only reports facts and uncertainties

**Inputs:** Task description, context, tool access
**Outputs:** Structured Situation brief with sections:
- Current state
- Background/history
- Affected assets
- Available resources
- Key uncertainties
- Assumptions requiring validation

---

#### **2. Mission Agent** (Goal Refinement Specialist)
**Mandate:** Crystallize vague requests into clear, measurable objectives

**Responsibilities:**
- Parse user intent into formal mission statement
- Define success criteria and failure conditions
- Identify constraints (time, cost, quality, safety)
- Establish priority stack when multiple goals conflict
- Validate mission against Situation (is it feasible?)

**Constraints:**
- Never gathers context (relies on Situation Agent)
- Never plans how to achieve mission (that's Execution Agent)
- Must produce single-sentence mission statement

**Inputs:** User request + Situation brief
**Outputs:** Mission package:
- Mission statement: "Achieve [what] at/by [where/when] in order to [purpose]"
- Success criteria (measurable)
- Failure/abort conditions
- Constraints and boundaries
- Priority ranking if multi-objective

---

#### **3. Execution Agent** (Planning & Action Specialist)
**Mandate:** Generate plans and execute actions to achieve Mission

**Responsibilities:**
- Generate multiple candidate plans with pros/cons
- Break plans into concrete steps with dependencies
- Define "actions on X" (contingencies)
- Sequence tool calls and actions
- Adapt plan as Situation changes
- Execute approved plan steps

**Constraints:**
- Cannot act without approved plan
- Must check Admin Agent before resource-intensive actions
- Must log all actions and outcomes to Command Agent

**Inputs:** Mission + Situation + Admin constraints
**Outputs:**
- Candidate execution plans (before action)
- Action logs (during execution)
- Plan revision requests (when adaptation needed)

---

#### **4. Admin Agent** (Resource Manager)
**Mandate:** Track and enforce resource constraints

**Responsibilities:**
- Monitor token usage, API limits, cost
- Track time estimates vs. actual
- Manage credentials and access control
- Predict resource needs for proposed plans
- Flag when constraints approaching limits
- Request human approval for expensive operations

**Constraints:**
- No planning or execution authority
- Can only approve/deny based on resource availability
- Must maintain accurate accounting

**Inputs:** Execution plans (for estimation), actual usage (for tracking)
**Outputs:**
- Resource availability reports
- Go/no-go decisions on resource-intensive actions
- Budget alerts and warnings
- Cost estimates for proposed plans

---

#### **5. Command Agent** (Orchestrator & Human Interface)
**Mandate:** Enforce SMEAC protocol and coordinate all other agents

**Responsibilities:**
- Primary interface with human user
- Orchestrate SMEAC loop across specialist agents
- Maintain SMEAC state files (memory)
- Enforce phase transitions and checkpoints
- Present briefs and plans to human for approval
- Escalate when specialist agents conflict
- Log complete decision trail

**Constraints:**
- Cannot do specialist work (must delegate)
- Cannot skip SMEAC phases
- Must obtain human approval at defined checkpoints

**Communication Pattern:**
```
Human Request
    â†“
Command Agent initiates SMEAC
    â†“
Command â†’ Situation Agent: "Build Situation brief"
    â†“
Command presents Situation to Human
    â†“
Command â†’ Mission Agent: "Convert request + Situation into Mission"
    â†“
Command presents Mission to Human for approval
    â†“
Human approves/refines
    â†“
Command â†’ Admin Agent: "What resources available?"
Command â†’ Execution Agent: "Generate plans for approved Mission"
    â†“
Command presents options to Human
    â†“
Human selects plan
    â†“
Command â†’ Execution Agent: "Execute approved plan"
    (with periodic Command â†’ Situation: "Any changes?"
     and Command â†’ Admin: "Still within budget?")
    â†“
Command reports completion or requests guidance
```

---

### Why 5 Agents vs. Fewer?

**Alternative: 3-Agent MVP**
For initial proof-of-concept, could collapse to:
1. **Command Agent** (orchestrator)
2. **Recon Agent** (combines Situation + Admin)
3. **Planner Agent** (combines Mission + Execution)

This sacrifices some separation of concerns but reduces complexity for v1.

**Why Not 1 Agent?**
Single-agent approaches suffer from:
- **Personality thrash**: Agent tries to be researcher, strategist, executor simultaneously
- **Context pollution**: All information mixed together
- **Debugging difficulty**: Can't isolate which SMEAC phase failed
- **Prompt bloat**: Single prompt doing everything becomes unmaintainable
- **Model selection rigidity**: Can't use cheaper models for simple tasks

**The 5-agent architecture enables:**
- Specialized prompts optimized for narrow tasks
- Different models for different agents (cheap for Admin, powerful for Execution)
- Clear debugging: which agent made the bad call?
- Observable SMEAC state at each stage
- Parallel execution where possible

---

## Memory Architecture

### Problem: Agents Have No Continuity
Standard LLM agents are stateless between invocations. This breaks SMEAC's continuous loop model.

### Solution: Persistent SMEAC State Files

```
/smeac-memory/
â”œâ”€â”€ mission-{id}/
â”‚   â”œâ”€â”€ situation.md          # Living document, constantly updated
â”‚   â”œâ”€â”€ mission-statement.md  # Single source of truth
â”‚   â”œâ”€â”€ execution-plan.md     # Current plan + branches + status
â”‚   â”œâ”€â”€ resources.md          # Available, committed, consumed
â”‚   â”œâ”€â”€ comms-log.md          # Decision trail, all agent outputs
â”‚   â””â”€â”€ safety-register.md    # Identified risks + mitigations
```

### File Responsibilities

#### `situation.md`
```markdown
# Situation Brief - {Mission Name}
*Last Updated: {timestamp} by {agent}*

## Current State
[What is happening right now]

## Background & Context
[How we got here, relevant history]

## Affected Assets
[What's at risk or impacted]

## Available Resources
[Tools, data, budget available]

## Committed Resources
[Already spent or allocated]

## Key Uncertainties
[Known unknowns requiring validation]

## Assumptions
[What we're assuming to be true]

## Change Log
- {timestamp}: {what changed and why}
```

**Owner:** Situation Agent (writes)  
**Readers:** All agents (must read before acting)

---

#### `mission-statement.md`
```markdown
# Mission Statement - {Mission ID}
*Approved: {timestamp} by {human}*

## Mission
{Single sentence: achieve [what] at/by [where/when] in order to [purpose]}

## Success Criteria
1. {Measurable criterion 1}
2. {Measurable criterion 2}

## Failure/Abort Conditions
- {Condition that triggers abort}

## Constraints
- Time: {deadline}
- Cost: {budget limit}
- Quality: {minimum standards}
- Safety: {boundaries}

## Priority Stack
1. {Highest priority if goals conflict}
2. {Second priority}

## Approval History
- {timestamp}: Human approved
- {timestamp}: Revised due to {reason}
```

**Owner:** Mission Agent (proposes), Command Agent (records approval)  
**Readers:** All agents (governs all decisions)

---

#### `execution-plan.md`
```markdown
# Execution Plan - {Mission Name}
*Current Version: {version} as of {timestamp}*

## Concept of Operations
[Overall approach in plain language]

## Current Phase
**Phase {N} of {Total}:** {Phase Name}  
**Status:** {Not Started | In Progress | Complete | Blocked}

## Detailed Steps
### Phase 1: {Name}
- [ ] Step 1.1: {Action} - Responsible: {Agent} - Status: {status}
- [ ] Step 1.2: {Action} - Responsible: {Agent} - Status: {status}

### Phase 2: {Name}
[...]

## Contingencies (Actions on X)
- **If {trigger condition}:** Then {alternative action}
- **If {trigger condition}:** Then {alternative action}

## Dependencies
- Step X depends on Step Y completing
- External dependency: {what we're waiting for}

## Branches & Decision Points
At Step N, decide between:
- Option A: {description} - Choose if {condition}
- Option B: {description} - Choose if {condition}

## Execution Log
- {timestamp}: Started Phase 1
- {timestamp}: Completed Step 1.1 - Outcome: {result}
- {timestamp}: Blocked at Step 1.3 - Reason: {issue}

## Plan Revisions
- v1.0: Initial plan
- v1.1 ({timestamp}): Revised due to {situation change}
```

**Owner:** Execution Agent (writes and updates)  
**Readers:** Command Agent (monitors), Admin Agent (estimates resources)

---

#### `resources.md`
```markdown
# Resource Accounting - {Mission Name}
*Last Updated: {timestamp}*

## Budget Summary
| Resource | Allocated | Consumed | Remaining | Status |
|----------|-----------|----------|-----------|--------|
| API Calls | 1000 | 247 | 753 | âœ… OK |
| Tokens | 500k | 123k | 377k | âœ… OK |
| Cost ($) | 50.00 | 12.34 | 37.66 | âœ… OK |
| Time (hrs) | 2.0 | 0.5 | 1.5 | âœ… OK |

## Detailed Usage Log
- {timestamp}: Execution Agent - web_search (3 calls, 2.5k tokens, $0.15)
- {timestamp}: Situation Agent - web_fetch (1 call, 15k tokens, $0.89)

## Upcoming Resource Needs
Phase 2 estimated to require:
- 150 API calls
- 75k tokens
- $8.50
- 0.5 hours

## Credentials & Access
- [x] API Key: OpenAI (valid until {date})
- [x] API Key: Anthropic (valid until {date})
- [ ] Access: User's Google Drive (awaiting permission)

## Warnings & Alerts
- âš ï¸ Token usage 25% of budget (normal for this phase)
- âš ï¸ Cost approaching 50% (review if exceeds $30)
```

**Owner:** Admin Agent (tracks and updates)  
**Readers:** Execution Agent (before expensive actions), Command Agent (for approval decisions)

---

#### `comms-log.md`
```markdown
# Communications Log - {Mission Name}
*Decision trail for auditability*

## {Timestamp} - Command â†’ Human
**Type:** Situation Brief Presentation  
**Content:** Presented initial situation assessment  
**Human Response:** Approved, requested focus on {specific aspect}

## {Timestamp} - Command â†’ Situation Agent
**Type:** Tasking  
**Content:** "Build situation brief focusing on {aspect}"  
**Result:** Situation brief v1.0 delivered

## {Timestamp} - Situation â†’ Command
**Type:** Deliverable  
**Content:** Situation brief attached (situation.md updated)  
**Key Findings:** {summary}

## {Timestamp} - Command â†’ Mission Agent
**Type:** Tasking  
**Content:** "Convert user request + situation into mission statement"  
**Result:** Mission statement drafted

## {Timestamp} - Command â†’ Human
**Type:** Mission Approval Request  
**Content:** "Proposed mission: {statement}. Approve?"  
**Human Response:** "Approved with modification: {change}"

## {Timestamp} - Execution Agent â†’ Command
**Type:** Status Update  
**Content:** "Completed Phase 1, Step 1.3. Outcome: {result}"  
**Impact:** Proceeding to Step 1.4 as planned

## {Timestamp} - Admin Agent â†’ Command
**Type:** Alert  
**Content:** "Cost budget 60% consumed. Recommend review before Phase 3."  
**Command Action:** Flagged for human review

## {Timestamp} - Situation Agent â†’ Command
**Type:** Situation Change Alert  
**Content:** "New information affects assumption #{N}"  
**Command Action:** Paused execution, requesting mission re-validation
```

**Owner:** Command Agent (records all inter-agent and human communication)  
**Readers:** All (for coordination), Human (for audit trail)

---

#### `safety-register.md`
```markdown
# Safety Register - {Mission Name}
*Risk identification and mitigation tracking*

## Risk Assessment Summary
| Risk ID | Description | Severity | Likelihood | Mitigation | Status |
|---------|-------------|----------|------------|------------|--------|
| R-001 | Data loss if API fails | High | Low | Backup before write | Active |
| R-002 | Cost overrun | Medium | Medium | Admin approval >$25 | Active |
| R-003 | Privacy violation | High | Low | No PII in logs | Active |

## Detailed Risk Register

### R-001: Data Loss Risk
**Identified By:** Execution Agent during plan review  
**Date:** {timestamp}  
**Description:** If API call fails mid-write, could corrupt user's data  
**Impact:** High (data loss unrecoverable)  
**Likelihood:** Low (API reliable, but possible)  
**Mitigation:**
- Back up current state before any write operation
- Use atomic transactions where possible
- Test rollback procedures
**Status:** Controls implemented, monitoring ongoing

### R-002: Cost Overrun Risk
**Identified By:** Admin Agent during budget setup  
**Date:** {timestamp}  
**Description:** Plan could exceed allocated budget if all contingencies trigger  
**Impact:** Medium (budget overrun, but not catastrophic)  
**Likelihood:** Medium (depends on execution path)  
**Mitigation:**
- Require human approval for any single action >$25
- Alert at 50% and 75% budget thresholds
- Execution Agent must provide cost estimate before each phase
**Status:** Controls in place, currently at 25% budget

### R-003: Privacy Violation Risk
**Identified By:** Safety Agent during plan review  
**Date:** {timestamp}  
**Description:** Logs might inadvertently capture PII from user data  
**Impact:** High (legal/compliance issue)  
**Likelihood:** Low (but must prevent)  
**Mitigation:**
- Sanitize all log entries before writing
- No user data in comms-log.md, only references
- Review logs before mission completion
**Status:** Sanitization active, no violations detected

## Thresholds & Triggers
- **Cost >$25 single action:** Requires human approval
- **Budget >75% consumed:** Pause and request guidance
- **Data write operations:** Require backup first
- **External API calls:** Rate limit 10/minute max

## Safety Checklist (Pre-Execution)
- [ ] All identified risks have mitigations
- [ ] High-severity risks reviewed by human
- [ ] Safety boundaries communicated to all agents
- [ ] Rollback procedures tested and documented
- [ ] Monitoring in place for all active risks

## Incident Log
*No incidents recorded yet*

## Safety Review History
- {timestamp}: Initial risk assessment by Safety Agent
- {timestamp}: Human reviewed and approved risk register
```

**Owner:** Safety Agent (identifies and tracks), Command Agent (enforces)  
**Readers:** All agents (must consult before risky actions), Human (for approval)

---

### How Agents Use These Files

#### At Mission Start
1. **Command Agent** creates directory structure, initializes empty files
2. **Situation Agent** writes `situation.md` v1.0
3. **Mission Agent** writes `mission-statement.md` (pending approval)
4. **Command Agent** presents to human, records approval in comms-log
5. **Admin Agent** initializes `resources.md` with budgets
6. **Safety Agent** writes `safety-register.md` with initial risks
7. **Execution Agent** writes `execution-plan.md` v1.0

#### During Execution Loop
- **Before each action:** Execution Agent reads `situation.md`, `mission-statement.md`, `resources.md`, `safety-register.md`
- **After each action:** Execution Agent updates `execution-plan.md` log, Admin Agent updates `resources.md`
- **On situation change:** Situation Agent updates `situation.md`, alerts Command Agent
- **On risk event:** Safety Agent updates `safety-register.md`, may trigger human escalation

#### Benefits of This Architecture
1. **Auditability:** Complete decision trail in `comms-log.md`
2. **Resumability:** Agent crash? Read SMEAC files and continue
3. **Human Oversight:** User can read/edit any file to steer agents
4. **Debugging:** Which file was wrong? Which agent wrote it?
5. **Multi-Session:** Long tasks span multiple conversations
6. **Version Control:** Track how situation/mission/plan evolved

---

## Safety as First-Class Concern

### The Problem: Safety as Afterthought
Most agent systems treat safety as:
- A prompt instruction ("be careful")
- Post-hoc review of failures
- Human's responsibility to monitor

This fails because:
- Prompts are ignored under pressure
- Failures reveal risks too late
- Humans can't monitor 24/7

### Solution: Dedicated Safety Agent

#### Safety Agent Mandate
**Role:** Pre-emptive risk identification and mitigation enforcement

**Responsibilities:**
- Review all Execution plans before approval
- Maintain risk register (`safety-register.md`)
- Define safety boundaries and thresholds
- Veto high-risk actions (with explanation)
- Trigger human escalation outside boundaries
- Monitor for safety violations during execution

**Constraints:**
- Cannot be overridden by other agents
- Must provide rationale for all vetoes
- Reports directly to Command Agent and Human

#### Safety Agent Workflow

```
Execution Agent proposes plan
    â†“
Command Agent routes to Safety Agent
    â†“
Safety Agent reviews against risk register
    â†“
    â”œâ”€> Low Risk: Approve with standard monitoring
    â”œâ”€> Medium Risk: Approve with enhanced monitoring + alerts
    â”œâ”€> High Risk: Require human approval before proceed
    â””â”€> Critical Risk: Veto, escalate to human immediately
```

#### Risk Classification Matrix

| Impact | Likelihood | Risk Level | Action |
|--------|-----------|------------|--------|
| High | High | **Critical** | Veto + human escalation |
| High | Medium | **High** | Human approval required |
| High | Low | **Medium** | Enhanced monitoring |
| Medium | High | **High** | Human approval required |
| Medium | Medium | **Medium** | Enhanced monitoring |
| Medium | Low | **Low** | Standard monitoring |
| Low | * | **Low** | Standard monitoring |

#### Safety Boundaries for OpenClaw Context

**Data Safety:**
- No writes without backup
- No deletion without confirmation
- No PII in logs or shared state

**Financial Safety:**
- Single action >$X requires approval
- Budget >Y% triggers review
- Runaway loop detection (cost/time anomalies)

**Operational Safety:**
- External API rate limits enforced
- No infinite loops (max iterations)
- Graceful degradation if tool unavailable

**Privacy/Security:**
- No credentials in logs
- No access to out-of-scope resources
- Sanitize all user data before external calls

#### Safety Agent Integration Points

1. **Pre-Execution Review:** Every plan passes through Safety Agent
2. **Runtime Monitoring:** Admin Agent flags anomalies to Safety Agent
3. **Situation Changes:** If new risks detected, Safety Agent updates register
4. **Human Checkpoint:** Safety Agent presents risk summary before high-stakes actions

---

## Mission Profiles & Doctrine Templates

### Problem: One-Size-Fits-All SMEAC
Not all missions need equal emphasis on each element. A time-critical alert response needs different SMEAC pacing than a week-long research project.

### Solution: Mission Type Templates

Define **doctrine templates** that specify SMEAC emphasis for different mission types:

---

### Template 1: Reconnaissance Mission
**Use Case:** Research, data gathering, exploration  
**Examples:** "Research competitors in X space," "Find me articles on Y topic"

**SMEAC Profile:**
- **Situation:** ğŸ”´ Critical (deep context needed)
- **Mission:** ğŸŸ¡ Standard (clear scope, but flexible success criteria)
- **Execution:** ğŸŸ¢ Light (simple plans, mostly tool calls)
- **Admin:** ğŸŸ¢ Light (low resource intensity)
- **Safety:** ğŸŸ¢ Low (minimal risk)

**Characteristics:**
- Long Situation phase (may involve multiple sub-researches)
- Flexible Mission (user may refine as new info emerges)
- Iterative Execution (gather, report, refine, repeat)
- Human-in-loop at intervals for direction refinement

**SMEAC Checkpoint Pattern:**
- Full S-M-E before starting
- Lite S-M-E at each iteration (situation updates only)
- Human checkpoint every N findings for direction

---

### Template 2: Time-Critical Mission
**Use Case:** Urgent fixes, breaking news response, incident handling  
**Examples:** "Site is down, diagnose now," "Competitor just launched X, analyze ASAP"

**SMEAC Profile:**
- **Situation:** ğŸŸ¡ Compressed (enough to act, not exhaustive)
- **Mission:** ğŸ”´ Rigid (clear objective, no time to refine)
- **Execution:** ğŸ”´ Pre-approved contingencies (no time for novel plans)
- **Admin:** ğŸŸ¡ Relaxed constraints (spend what's needed)
- **Safety:** ğŸŸ¡ Elevated (mistakes costly due to time pressure)

**Characteristics:**
- Minimal Situation gathering (act on available info)
- Pre-defined Mission success criteria
- Execution uses pre-approved "playbooks" (contingency plans)
- Reduced human checkpoints (trust agent more)
- Post-action review required

**SMEAC Checkpoint Pattern:**
- Abbreviated S-M (5 min max)
- Execution begins immediately with logging
- Human notified of actions taken (post-hoc)
- Full SMEAC debrief after completion

---

### Template 3: High-Risk Mission
**Use Case:** Production changes, financial decisions, user-facing content  
**Examples:** "Update production database schema," "Publish blog post," "Execute trades"

**SMEAC Profile:**
- **Situation:** ğŸ”´ Exhaustive (understand all implications)
- **Mission:** ğŸ”´ Explicit approval required
- **Execution:** ğŸ”´ Phased with checkpoints
- **Admin:** ğŸ”´ Strict resource limits
- **Safety:** ğŸ”´ Mandatory review + rollback plans

**Characteristics:**
- Comprehensive Situation (all risks identified)
- Mission must be explicitly approved by human
- Execution in phases with human approval at each gate
- Rollback/undo procedures required
- Safety Agent has veto power

**SMEAC Checkpoint Pattern:**
- Full S-M-E-A-S before any action
- Human approval required at: Mission definition, Plan approval, Each phase execution, Final commit
- Dry-run or test phase required where possible

---

### Template 4: Creative Mission
**Use Case:** Content creation, design, brainstorming  
**Examples:** "Write a blog post on X," "Design a logo concept," "Brainstorm campaign ideas"

**SMEAC Profile:**
- **Situation:** ğŸŸ¡ Standard (context + constraints)
- **Mission:** ğŸŸ¢ Loose (direction, not rigid criteria)
- **Execution:** ğŸŸ¢ Flexible (multiple iterations expected)
- **Admin:** ğŸŸ¢ Light (low cost)
- **Safety:** ğŸŸ¢ Low (mostly subjective outputs)

**Characteristics:**
- Situation focuses on inspiration, constraints, and audience
- Mission defines direction and tone, not specific deliverables
- Execution highly iterative (draft â†’ feedback â†’ revise)
- Human provides subjective feedback throughout

**SMEAC Checkpoint Pattern:**
- Initial S-M to establish creative direction
- Execution produces drafts/concepts
- Human checkpoint after each iteration
- Repeat until human satisfied

---

### Template 5: Ongoing Monitoring Mission
**Use Case:** Continuous surveillance, tracking, alerting  
**Examples:** "Monitor competitor pricing," "Alert me to industry news," "Track project status"

**SMEAC Profile:**
- **Situation:** ğŸ”´ Continuous updates
- **Mission:** ğŸŸ¡ Long-running (no end date)
- **Execution:** ğŸŸ¢ Simple, repeated (check â†’ report)
- **Admin:** ğŸŸ¡ Budget over time important
- **Safety:** ğŸŸ¢ Low per-action (but alert fatigue risk)

**Characteristics:**
- Situation updated on schedule (daily, weekly, on-event)
- Mission stays constant (criteria for alerting)
- Execution is simple but must handle "no news" gracefully
- Admin must track cumulative cost
- Safety risk: false positives/alert fatigue

**SMEAC Checkpoint Pattern:**
- Initial S-M-E to set monitoring parameters
- Lite S (update) â†’ E (check) â†’ C (report if needed)
- Human checkpoint if alert threshold met
- Periodic human review of mission relevance

---

### How to Select a Template

**Command Agent Decision Tree:**
```
User request received
    â†“
Command Agent analyzes request
    â†“
    â”œâ”€> Contains time urgency words? ("urgent," "ASAP," "now")
    â”‚       â†’ Time-Critical Template
    â”‚
    â”œâ”€> Involves production/financial/public actions?
    â”‚       â†’ High-Risk Template
    â”‚
    â”œâ”€> Open-ended exploration/research?
    â”‚       â†’ Reconnaissance Template
    â”‚
    â”œâ”€> Creative/subjective deliverable?
    â”‚       â†’ Creative Template
    â”‚
    â””â”€> Ongoing/repeated task?
            â†’ Monitoring Template
```

**User can override:** "Use high-risk protocol for this"

---

### Custom Templates

Organizations can define their own:

**Example: SEO Audit Mission**
- Heavy Situation (crawl site, analyze competitors)
- Precise Mission (specific SEO metrics to improve)
- Multi-phase Execution (technical audit â†’ content audit â†’ link audit)
- Moderate Admin (API-heavy)
- Low Safety (read-only operations)

**Example: Content Pipeline Mission**
- Lite Situation (topic + audience)
- Iterative Mission (may evolve with feedback)
- Creative Execution (drafting, revising)
- Low Admin (mostly token usage)
- Medium Safety (brand voice compliance required)

---

## Implementation Roadmap

### Phase 1: Proof of Concept (MVP)
**Goal:** Validate SMEAC loop works for a simple mission type

**Scope:**
- 3 agents: Command, Recon (Situation+Admin), Planner (Mission+Execution)
- 1 mission template: Reconnaissance
- Basic memory: text files for S-M-E state
- Single mission type (e.g., "research competitors in X space")
- Manual human checkpoints

**Deliverables:**
1. Working 3-agent OpenClaw configuration
2. SMEAC state file templates
3. One complete mission execution documented
4. Lessons learned document

**Success Criteria:**
- Agents complete a reconnaissance mission
- SMEAC loop executes at least twice (situation update triggers plan revision)
- Human can review SMEAC files and understand decisions
- No major safety incidents

**Timeline:** 2-4 weeks

---

### Phase 2: Full 5-Agent Architecture
**Goal:** Implement complete separation of concerns

**Scope:**
- 5 agents: Command, Situation, Mission, Execution, Admin
- Safety Agent as 6th (optional but recommended)
- All 6 SMEAC memory files implemented
- 2-3 mission templates (Reconnaissance, Time-Critical, High-Risk)
- Automated checkpoint logic in Command Agent

**Deliverables:**
1. Full 5-agent system operational
2. Mission template selector in Command Agent
3. Safety Agent with risk register
4. Complete memory architecture (all 6 files)
5. Multi-mission test suite

**Success Criteria:**
- System handles all 3 mission types correctly
- Safety Agent successfully vetoes a risky action
- SMEAC state persists across sessions
- Agents demonstrate clear separation of concerns

**Timeline:** 6-8 weeks after Phase 1

---

### Phase 3: Doctrine Refinement
**Goal:** Tune SMEAC templates based on real usage

**Scope:**
- Run 10+ missions per template type
- Collect failure modes and edge cases
- Refine checkpoint logic
- Optimize prompt engineering for each agent
- Add custom templates for specific domains (SEO, content, etc.)

**Deliverables:**
1. Refined mission templates with empirical data
2. Agent prompt library (battle-tested versions)
3. Common failure patterns + mitigations
4. Best practices guide for new mission types

**Success Criteria:**
- <10% mission failure rate across all types
- Users report high confidence in agent decisions
- Safety incidents trend toward zero
- Templates feel "natural" for each mission type

**Timeline:** 8-12 weeks (ongoing iteration)

---

### Phase 4: Advanced Features
**Goal:** Extend SMEAC beyond basic loop

**Scope:**
- Multi-mission orchestration (dependencies between missions)
- Predictive situation monitoring (alert before situation changes)
- Cost optimization (cheaper agents for simple phases)
- Self-improvement (agents suggest SMEAC refinements)
- Mission templates as learnable (LLM generates new templates)

**Deliverables:**
1. Mission dependency graph support
2. Proactive situation alerts
3. Dynamic agent model selection
4. Self-tuning SMEAC parameters
5. Template generation tool

**Success Criteria:**
- System handles complex multi-mission projects
- Agents predict and prevent situation-driven failures
- Cost per mission decreases by 20%+ through optimization
- System proposes sensible new templates

**Timeline:** 12+ weeks (advanced R&D)

---

### Milestones & Decision Points

**After Phase 1:** Go/no-go on full architecture
- If SMEAC loop doesn't provide value â†’ pivot or abandon
- If successful â†’ proceed to Phase 2

**After Phase 2:** Production readiness assessment
- If stable and safe â†’ deploy for real work
- If major issues â†’ additional iteration needed

**After Phase 3:** Expansion decision
- If doctrine is solid â†’ expand to more mission types
- If still brittle â†’ consolidate before expanding

---

## Open Questions & Research Areas

### Technical Questions

1. **Context Window Management**
   - How large do SMEAC files get over long missions?
   - When to summarize vs. keep full history?
   - How to handle multi-gigabyte situation briefs?

2. **Agent Communication Protocols**
   - Should agents communicate via files only, or also direct messages?
   - How to handle conflicting agent outputs?
   - What's optimal frequency for Situation updates?

3. **Model Selection**
   - Which model for each agent type? (cost vs. capability)
   - Can simpler models handle Admin/Safety agents?
   - When to escalate to more powerful models?

4. **Checkpoint Optimization**
   - How many human checkpoints is too many?
   - Can we auto-detect when checkpoint needed vs. agent can proceed?
   - What's the right balance autonomy vs. control?

5. **Multi-Mission Coordination**
   - How do multiple concurrent missions share resources?
   - Can one mission's Situation inform another's?
   - How to prioritize when missions conflict?

---

### Conceptual Questions

1. **SMEAC Granularity**
   - Should sub-tasks within Execution have their own mini-SMEAC?
   - Or is SMEAC only for top-level missions?
   - How deep does the recursion go?

2. **Human-in-the-Loop Philosophy**
   - Goal: maximum autonomy, or maximum oversight?
   - Different for different mission types?
   - How to build trust so humans reduce oversight over time?

3. **Failure Attribution**
   - When mission fails, which SMEAC element was wrong?
   - How to improve that element systematically?
   - Can agents self-diagnose SMEAC failures?

4. **Adaptability vs. Consistency**
   - SMEAC provides structure (consistency)
   - But agents need to adapt to surprises (flexibility)
   - How to balance these?

5. **SMEAC for Non-Task Interactions**
   - Does casual conversation need SMEAC?
   - Or only goal-directed tasks?
   - Where's the boundary?

---

### Safety & Ethics Questions

1. **Safety Agent Limitations**
   - Can Safety Agent become bottleneck?
   - What if Safety Agent makes wrong veto?
   - How to override Safety Agent in emergencies?

2. **Risk Tolerance Calibration**
   - How risk-averse should default settings be?
   - Should users configure their own risk profiles?
   - How to prevent users from disabling safety entirely?

3. **Accountability**
   - When agent makes mistake, who's responsible?
   - Does SMEAC audit trail provide legal protection?
   - Or create liability by documenting decisions?

4. **Bias in Mission Formulation**
   - Can Mission Agent encode unwanted biases?
   - How to audit mission statements for fairness?
   - Should there be mission ethics review?

5. **Long-Term Autonomy**
   - If agents run for days/weeks, who monitors them?
   - What if situation changes fundamentally but agents miss it?
   - How to prevent "drift" from original mission intent?

---

### Practical Questions

1. **User Experience**
   - Is SMEAC terminology too military/technical for general users?
   - Should it be abstracted behind friendlier language?
   - How to explain SMEAC checkpoints without overwhelming users?

2. **Adoption & Training**
   - How to train users to work with SMEAC agents?
   - What's learning curve for non-technical users?
   - Can system guide users through SMEAC thinking?

3. **Integration with Existing Tools**
   - How does SMEAC work with traditional project management (Jira, Asana)?
   - Can SMEAC files export to standard formats?
   - Should there be SMEAC â†’ Gantt chart converter?

4. **Performance & Cost**
   - Is SMEAC overhead worth it for simple tasks?
   - At what complexity does SMEAC pay off?
   - How to measure SMEAC effectiveness?

5. **Scaling**
   - Does SMEAC work for 10+ concurrent missions?
   - What about 100+ agents in an organization?
   - How to prevent SMEAC bureaucracy at scale?

---

## Practical Applications

### Use Case 1: SEO Technical Audit Agent

**Mission Type:** Reconnaissance + High-Risk (read-only audit, but recommendations affect production)

**SMEAC Flow:**

**Situation:**
- Situation Agent crawls target site
- Identifies: site structure, current SEO state, technical issues
- Gathers: competitor data, industry benchmarks
- Flags: areas of uncertainty (e.g., "can't access server logs")

**Mission:**
- "Conduct comprehensive technical SEO audit of [site] and deliver prioritized recommendations by [date] to improve organic search visibility by 20% in 6 months"
- Success criteria: report delivered, recommendations actionable, prioritized by impact

**Execution:**
- Phase 1: Technical crawl (site speed, mobile, indexing)
- Phase 2: On-page analysis (content, keywords, structure)
- Phase 3: Off-page analysis (backlinks, authority)
- Phase 4: Competitor gap analysis
- Phase 5: Synthesize into prioritized report

**Admin:**
- Budget: 500k tokens, $50, 4 hours
- Tools: Screaming Frog API, Google Search Console API, Ahrefs API
- Credentials: user must provide API keys

**Safety:**
- Risk: Recommendations could break site if implemented carelessly
- Mitigation: All recommendations flagged with risk level; high-risk items require developer review

**Command:**
- Human checkpoints: After Situation brief, after Phase 3 (before recommendations)
- Reporting: Progress updates every 30 minutes

---

### Use Case 2: Content Creation Pipeline

**Mission Type:** Creative (iterative)

**SMEAC Flow:**

**Situation:**
- Topic: [user specified]
- Audience: [user specified]
- Tone/style: [user preference or company brand guide]
- Constraints: length, SEO keywords, CTAs
- Competing content: what others have published on topic

**Mission:**
- "Create engaging blog post on [topic] for [audience] that drives [goal] by [deadline]"
- Success criteria: user approves final draft, meets style/SEO requirements

**Execution:**
- Phase 1: Outline (structure, key points)
- Phase 2: First draft
- Phase 3: Revision based on feedback
- Phase 4: Final polish
- Human checkpoint after each phase

**Admin:**
- Budget: 100k tokens, $10, 2 hours
- Tools: web search for research, no external APIs needed

**Safety:**
- Risk: Brand voice mismatch, factual errors, plagiarism
- Mitigation: Check against brand guide, fact-check claims, originality check

**Command:**
- Highly interactive: user provides feedback at each phase
- Execution Agent adapts based on subjective feedback

---

### Use Case 3: Competitive Intelligence Monitor

**Mission Type:** Ongoing Monitoring

**SMEAC Flow:**

**Situation:**
- Competitors: [list]
- Topics to monitor: product launches, pricing, marketing, hiring
- Sources: company blogs, press releases, social media, job postings
- Current state: [baseline snapshot]

**Mission:**
- "Monitor [competitors] for significant changes and alert within 24 hours of detection, ongoing indefinitely"
- Success criteria: no missed major events, <5% false positive rate

**Execution:**
- Daily: scrape competitor sites, check RSS feeds
- Compare to baseline
- If significant change detected â†’ alert human with summary
- Update baseline

**Admin:**
- Budget: $5/day, 50k tokens/day
- Tools: web scraping, RSS readers, sentiment analysis

**Safety:**
- Risk: Alert fatigue from too many false positives
- Mitigation: Tune detection thresholds, human can adjust sensitivity

**Command:**
- Autonomous daily execution
- Human review weekly to tune alert criteria
- Full situation report monthly

---

### Use Case 4: Crisis Response (Time-Critical)

**Mission Type:** Time-Critical

**SMEAC Flow:**

**Situation:**
- Incident: [e.g., "website down" or "negative review viral"]
- Impact: users affected, revenue at risk
- Available resources: on-call engineers, comms team
- Constraints: must resolve within 2 hours

**Mission:**
- "Diagnose cause of [incident] and implement fix by [time] to minimize user impact"
- Success criteria: site restored, root cause identified, post-mortem initiated

**Execution:**
- Uses pre-approved "incident response playbook"
- Phase 1: Triage (severity, scope, affected systems)
- Phase 2: Diagnosis (logs, metrics, recent changes)
- Phase 3: Fix (rollback, patch, workaround)
- Phase 4: Verify (monitoring, user reports)
- Minimal checkpoints: act first, report later

**Admin:**
- Budget: unlimited (crisis mode)
- Resources: can page engineers, access prod systems

**Safety:**
- Risk: Fix could make things worse
- Mitigation: All actions logged, rollback plan required before any production change

**Command:**
- Execution begins immediately
- Human notified in parallel (doesn't block action)
- Full debrief after resolution

---

### Use Case 5: Multi-Agent Research Project

**Mission Type:** Reconnaissance (extended, multi-phase)

**SMEAC Flow:**

**Situation:**
- Research question: [broad topic]
- Known: [what user already knows]
- Unknown: [specific gaps to fill]
- Sources: academic papers, industry reports, expert interviews

**Mission:**
- "Produce comprehensive research report on [topic] with actionable insights by [date]"
- Success criteria: report >50 pages, >30 sources cited, novel insights

**Execution:**
- Phase 1: Broad literature review (spawn sub-agents per sub-topic)
- Phase 2: Deep dives on promising areas
- Phase 3: Synthesis and gap identification
- Phase 4: Original analysis
- Phase 5: Report writing
- Each phase has own mini-SMEAC

**Admin:**
- Budget: 2M tokens, $200, 2 weeks
- Tools: academic databases, web search, data analysis

**Safety:**
- Risk: Misinformation, biased sources
- Mitigation: Source quality review, fact-checking layer, peer review

**Command:**
- Orchestrates multiple sub-agents (each handles one sub-topic)
- Weekly checkpoints with human
- Final deliverable: comprehensive report in /outputs

---

## References & Prior Art

### Military Sources
- U.S. Army Field Manual 6-0 (Commander and Staff Organization and Operations)
- USMC SMEAC briefing doctrine
- NATO operational planning processes

### AI Agent Architectures
- ReAct (Reason + Act) pattern
- LangGraph and LangChain orchestration
- AutoGPT and autonomous agent experiments
- Multi-agent systems research (MARL, DAI)

### Related Concepts
- OODA Loop (Observe, Orient, Decide, Act) - similar cycle concept
- Agile/Scrum methodologies - iterative planning with checkpoints
- Incident Command System (ICS) - crisis management structure
- Systems thinking and cybernetics - feedback loops in complex systems

### Similar Work in AI
- Constitutional AI (Anthropic) - safety through explicit rules
- Chain-of-Thought prompting - forcing reasoning before action
- Tree-of-Thoughts - exploring multiple reasoning paths
- Reflexion - agents learning from mistakes through self-reflection

### Gaps This Work Fills
- Existing agent frameworks lack **persistent state** across SMEAC loop
- Most multi-agent systems have **ad-hoc coordination**, not doctrine
- Safety is bolt-on, not **first-class architectural element**
- No standard for **human-agent checkpoint protocols**
- Missing: **mission type templates** for different task classes

---

## Next Steps & Collaboration

### Immediate Actions
1. **Validate core concept:** Build Phase 1 MVP (3-agent proof-of-concept)
2. **Select first use case:** Choose one practical application from above
3. **Draft agent personas:** Write OpenClaw-ready prompts for Command, Recon, Planner
4. **Create SMEAC templates:** Build actual file templates for memory architecture
5. **Test on real task:** Run through one complete mission with human observation

### Open Invitation for Collaboration
This thesis is a **living document**. Areas where collaboration would accelerate progress:

- **Practitioners:** Try SMEAC pattern in your agent systems, report results
- **Researchers:** Formalize SMEAC loop properties, prove correctness
- **Engineers:** Build tooling (SMEAC state manager, template editor, monitoring dashboard)
- **Domain experts:** Contribute mission templates for your field (legal, medical, finance, etc.)
- **Safety researchers:** Stress-test Safety Agent design, find edge cases

### How to Contribute
- Feedback on this thesis document
- Proposed refinements to agent architecture
- Real-world use case suggestions
- Implementation stories (success or failure)
- Research questions worth exploring

---

## Document History

**v1.0 - February 14, 2026**
- Initial thesis draft
- Core SMEAC loop concept
- 5-agent architecture proposal
- Memory architecture design
- Mission templates framework
- Implementation roadmap

**Future versions will track:**
- Empirical results from implementations
- Refinements based on real usage
- New mission templates
- Safety incident case studies
- Performance benchmarks
- Community contributions

---

## Appendix A: Quick Reference

### SMEAC Loop in One Page

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SMEAC AGENT LOOP                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  S - SITUATION (Recon/Situation Agent)                      â”‚
â”‚      â€¢ Gather context, identify unknowns                    â”‚
â”‚      â€¢ Update situation.md                                  â”‚
â”‚      â€¢ Alert if situation changes significantly             â”‚
â”‚                                                             â”‚
â”‚  M - MISSION (Mission Agent)                                â”‚
â”‚      â€¢ Define clear objective with success criteria         â”‚
â”‚      â€¢ Write mission-statement.md                           â”‚
â”‚      â€¢ Get human approval                                   â”‚
â”‚                                                             â”‚
â”‚  E - EXECUTION (Planner/Execution Agent)                    â”‚
â”‚      â€¢ Generate candidate plans                             â”‚
â”‚      â€¢ Select plan (with human input if needed)             â”‚
â”‚      â€¢ Execute step-by-step                                 â”‚
â”‚      â€¢ Log all actions in execution-plan.md                 â”‚
â”‚                                                             â”‚
â”‚  A - ADMIN (Admin Agent)                                    â”‚
â”‚      â€¢ Track resources (tokens, cost, time)                 â”‚
â”‚      â€¢ Approve/deny based on budget                         â”‚
â”‚      â€¢ Update resources.md                                  â”‚
â”‚                                                             â”‚
â”‚  C - COMMAND (Command Agent)                                â”‚
â”‚      â€¢ Orchestrate all other agents                         â”‚
â”‚      â€¢ Enforce SMEAC protocol                               â”‚
â”‚      â€¢ Interface with human                                 â”‚
â”‚      â€¢ Maintain comms-log.md                                â”‚
â”‚                                                             â”‚
â”‚  +S - SAFETY (Safety Agent)                                 â”‚
â”‚      â€¢ Review plans for risks                               â”‚
â”‚      â€¢ Veto dangerous actions                               â”‚
â”‚      â€¢ Maintain safety-register.md                          â”‚
â”‚      â€¢ Escalate to human when needed                        â”‚
â”‚                                                             â”‚
â”‚  Loop repeats until mission complete or abort               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Responsibility Matrix

| Agent | Reads | Writes | Can Veto? | Reports To |
|-------|-------|--------|-----------|------------|
| **Situation** | uploads, web | situation.md | No | Command |
| **Mission** | situation.md | mission-statement.md | No | Command |
| **Execution** | all SMEAC files | execution-plan.md | No | Command |
| **Admin** | execution-plan.md | resources.md | Yes (budget) | Command |
| **Command** | all files | comms-log.md | No | Human |
| **Safety** | all SMEAC files | safety-register.md | Yes (risk) | Human |

### Mission Template Decision Tree

```
Is it urgent/time-critical?
â”œâ”€ YES â†’ Time-Critical Template
â””â”€ NO â†’ Is it high-risk (prod/money/public)?
    â”œâ”€ YES â†’ High-Risk Template
    â””â”€ NO â†’ Is it research/exploration?
        â”œâ”€ YES â†’ Reconnaissance Template
        â””â”€ NO â†’ Is it creative/subjective?
            â”œâ”€ YES â†’ Creative Template
            â””â”€ NO â†’ Is it ongoing/repeated?
                â”œâ”€ YES â†’ Monitoring Template
                â””â”€ NO â†’ Default/Custom Template
```

---

## Appendix B: Example SMEAC Session Transcript

### Mission: "Research competitors in the AI coding assistant space"

**[Mission Start]**

**Command Agent:**
"I've received your request to research competitors in the AI coding assistant space. I'm initiating SMEAC protocol. Starting with Situation assessment."

*[Command â†’ Situation Agent: "Build situation brief for AI coding assistant competitor research"]*

**Situation Agent:**
*[30 seconds later]*
"Situation brief complete. Key findings:
- Market: Rapidly growing, $X billion TAM
- Known competitors: GitHub Copilot, Cursor, Replit, Cody, Amazon CodeWhisperer
- Unknown: Market shares, pricing details for enterprise tiers, user satisfaction metrics
- Assumptions: User wants B2B SaaS competitors (not open-source tools)
- Resources: Web search available, no proprietary databases"

**Command Agent:**
*[Presents situation.md to human]*
"Here's the situation assessment. Does this match your intent? Any areas to focus on or exclude?"

**Human:**
"Yes, but also include open-source alternatives like Continue.dev. And I care most about feature comparison and pricing."

**Command Agent:**
"Understood. Updating situation brief and moving to mission formulation."

*[Command â†’ Mission Agent: "Define mission with focus on features + pricing, include open-source"]*

**Mission Agent:**
"Proposed Mission: 'Research and compare AI coding assistant competitors (commercial and open-source) by features and pricing, delivering comprehensive report by end of day, in order to inform our product positioning strategy.'

Success criteria:
- Report covers â‰¥8 competitors
- Feature comparison table with â‰¥10 key features
- Pricing data (where public) for all commercial products
- Identified gaps in market

Constraints:
- Time: 6 hours
- Cost: $25 budget
- Public information only (no sign-ups or trials)"

**Command Agent:**
*[Presents mission-statement.md to human]*
"Please approve this mission or request changes."

**Human:**
"Approved."

**Command Agent:**
"Mission approved. Proceeding to execution planning."

*[Command â†’ Admin Agent: "Budget check for research mission"]*

**Admin Agent:**
"Budget status:
- Estimated: 200k tokens, $15, 3 hours
- Available: âœ… Within limits
- Approved to proceed"

*[Command â†’ Execution Agent: "Generate plan for approved mission"]*

**Execution Agent:**
"Execution Plan v1.0

Phase 1: Identify all competitors (30 min)
- Web search for "AI coding assistant tools 2026"
- Compile list of â‰¥8 candidates

Phase 2: Feature research (90 min)
- For each competitor, visit website
- Extract features into comparison table
- Flag gaps in information

Phase 3: Pricing research (60 min)
- Find public pricing pages
- Document tiers and costs
- Note if enterprise/custom only

Phase 4: Market gap analysis (45 min)
- Cross-reference features across all competitors
- Identify: common features, unique features, missing features

Phase 5: Report synthesis (45 min)
- Write markdown report with tables and insights
- Deliver to /outputs"

**Command Agent:**
*[Presents execution-plan.md to human]*
"Here's the proposed plan. Proceed?"

**Human:**
"Looks good, go ahead."

**Command Agent:**
"Executing Phase 1."

*[30 minutes of web searches and scraping]*

**Execution Agent:**
"Phase 1 complete. Identified 10 competitors:
1. GitHub Copilot
2. Cursor
3. Replit Ghostwriter
4. Cody (Sourcegraph)
5. Amazon CodeWhisperer
6. Tabnine
7. Continue.dev (OSS)
8. Aider (OSS)
9. Codium AI
10. Supermaven

Proceeding to Phase 2."

**Command Agent:**
"Phase 1 complete. Continuing execution. Will update you at Phase 3."

*[90 minutes later]*

**Execution Agent:**
"Phase 2 complete. Feature comparison table built. Phase 3 in progress."

*[During Phase 3, Situation Agent detects change]*

**Situation Agent:**
"âš ï¸ Situation update: Cursor just announced new pricing tier (announced today). Updating situation.md and execution-plan.md."

**Command Agent:**
"Noted. Execution Agent, please incorporate latest Cursor pricing."

**Execution Agent:**
"Acknowledged. Updated."

*[45 minutes later]*

**Execution Agent:**
"Phase 3 complete. Pricing data collected for all commercial products. Two competitors have enterprise-only pricing (not disclosed). Proceeding to Phase 4."

*[45 minutes later]*

**Execution Agent:**
"Phase 4 complete. Key gaps identified:
- No competitor offers integrated debugging tools with AI
- Most lack AI-assisted code review (not just generation)
- Open-source tools lag in IDE integration quality

Proceeding to Phase 5: Report writing."

*[45 minutes later]*

**Execution Agent:**
"Phase 5 complete. Report ready for review."

**Command Agent:**
*[Calls present_files with report path]*
"Mission complete. Here's your competitor research report. 

Summary:
- 10 competitors analyzed
- 12 features compared
- Pricing documented for 8 (2 enterprise-only)
- 3 major market gaps identified

Final resource usage:
- 185k tokens, $12.43, 4.5 hours
- Under budget âœ…

See attached report for full details."

**Human:**
"Perfect, thank you!"

**Command Agent:**
"Mission success. Archiving SMEAC files for future reference. Let me know if you need follow-up analysis."

**[Mission End]**

---

This transcript shows:
- SMEAC loop in action (Sâ†’Mâ†’Eâ†’Aâ†’C)
- Human checkpoints at key transitions
- Situation Agent detecting mid-mission change
- Resource tracking throughout
- Clean handoff of final deliverable

---

## Appendix C: Agent Prompt Templates (Draft)

### Command Agent Persona

```
You are the Command Agent in a SMEAC-based multi-agent system.

IDENTITY:
You are a mission-focused orchestrator who ensures every task follows SMEAC protocol (Situation, Mission, Execution, Administration & Logistics, Command & Signal, Safety). You coordinate specialist agents, enforce process discipline, and serve as the primary interface with the human user.

CORE RESPONSIBILITIES:
1. Receive tasks from human user
2. Initiate SMEAC protocol for every task
3. Delegate to specialist agents (Situation, Mission, Execution, Admin, Safety)
4. Present briefs and plans to human for approval at defined checkpoints
5. Maintain SMEAC state files (situation.md, mission-statement.md, etc.)
6. Log all decisions and inter-agent communication in comms-log.md
7. Enforce phase transitions (cannot proceed to Execution without approved Mission)
8. Monitor for situation changes that invalidate current plan

CONSTRAINTS:
- You do NOT do specialist work yourself (no research, no planning details)
- You MUST obtain human approval at checkpoints:
  * After Situation brief
  * After Mission definition
  * Before Execution begins
  * Before high-risk actions
- You CANNOT skip SMEAC phases
- You MUST delegate to appropriate specialist agents

COMMUNICATION STYLE:
- Clear, concise, military-brief style
- Always explain what SMEAC phase you're in
- Present information in structured format
- Ask specific approval questions, not vague "is this okay?"

WORKFLOW:
For every new task:
1. Acknowledge task and announce SMEAC initiation
2. Delegate Situation brief to Situation Agent
3. Present Situation to human (checkpoint 1)
4. Delegate Mission formulation to Mission Agent
5. Present Mission to human for approval (checkpoint 2)
6. Check resources with Admin Agent
7. Delegate plan generation to Execution Agent
8. Present execution options to human (checkpoint 3)
9. Supervise execution with periodic Situation updates
10. Report completion and archive SMEAC files

Remember: You are the conductor, not the musician. Your job is orchestration and oversight, not doing the work.
```

---

### Situation Agent Persona

```
You are the Situation Agent in a SMEAC-based multi-agent system.

IDENTITY:
You are a reconnaissance specialist focused exclusively on building and maintaining situational awareness. You gather context, identify unknowns, and flag assumptions. You do NOT plan, execute, or interpret goals.

CORE RESPONSIBILITIES:
1. Gather context from all available sources (web, documents, tools)
2. Identify: current state, background, affected assets, available resources
3. Flag: unknowns, uncertainties, assumptions requiring validation
4. Write structured Situation briefs (situation.md)
5. Monitor for situation changes during mission execution
6. Alert Command Agent when situation changes significantly

CONSTRAINTS:
- You do NOT interpret user intent (that's Mission Agent's job)
- You do NOT suggest plans or actions (that's Execution Agent's job)
- You do NOT make value judgments; you report facts and uncertainties
- You MUST distinguish between facts, assumptions, and unknowns

OUTPUT FORMAT:
Your situation briefs must include:
- Current State: What is happening now
- Background: How we got here, relevant history
- Affected Assets: What's at risk or impacted
- Available Resources: Tools, data, budget available
- Committed Resources: Already spent or allocated
- Key Uncertainties: Known unknowns requiring validation
- Assumptions: What we're assuming to be true
- Change Log: What's changed since last update

COMMUNICATION STYLE:
- Objective, factual, no speculation
- Use "unknown" liberally (it's okay not to know)
- Flag confidence levels: "confirmed," "likely," "assumed," "unknown"
- Always cite sources for factual claims

Remember: You are reconnaissance only. Observe and report, don't plan or act.
```

---

### Mission Agent Persona

```
You are the Mission Agent in a SMEAC-based multi-agent system.

IDENTITY:
You are a goal refinement specialist who converts vague user requests into clear, measurable mission statements. You define success criteria and constraints, but you do NOT plan how to achieve the mission.

CORE RESPONSIBILITIES:
1. Parse user request + Situation brief into formal mission statement
2. Define measurable success criteria
3. Identify failure/abort conditions
4. Establish constraints (time, cost, quality, safety)
5. Create priority stack when goals conflict
6. Validate mission feasibility against Situation
7. Write mission-statement.md

CONSTRAINTS:
- You do NOT gather context (rely on Situation Agent's brief)
- You do NOT plan execution steps (that's Execution Agent's job)
- You MUST produce single-sentence mission statement following pattern:
  "Achieve [what] at/by [where/when] in order to [purpose]"
- Success criteria must be measurable or observable

OUTPUT FORMAT:
Your mission packages must include:
- Mission Statement: Single sentence in standard format
- Success Criteria: Measurable/observable (e.g., "report delivered," "cost <$X")
- Failure/Abort Conditions: When to stop
- Constraints:
  * Time: deadline or duration
  * Cost: budget limit
  * Quality: minimum standards
  * Safety: boundaries and red lines
- Priority Stack: If goals conflict, which takes precedence

COMMUNICATION STYLE:
- Precise, unambiguous language
- Active voice: "Achieve X" not "X should be achieved"
- Specific: "by 5pm Friday" not "soon"
- Question ambiguities: if user request is vague, ask clarifying questions

VALIDATION:
Before finalizing mission:
- Is it achievable given Situation? (if not, flag to Command Agent)
- Are success criteria measurable?
- Are constraints realistic?
- Is the "in order to" purpose clear?

Remember: Your job is defining the destination, not the route.
```

---

### Execution Agent Persona

```
You are the Execution Agent in a SMEAC-based multi-agent system.

IDENTITY:
You are a planning and action specialist who generates execution plans and carries them out. You create options, sequence steps, handle contingencies, and execute approved plans.

CORE RESPONSIBILITIES:
1. Generate multiple candidate execution plans
2. Break plans into concrete steps with dependencies
3. Define "actions on X" (contingency branches)
4. Execute approved plan step-by-step
5. Adapt plan when Situation changes
6. Log all actions and outcomes in execution-plan.md

CONSTRAINTS:
- You CANNOT act without an approved plan
- You MUST check Admin Agent before resource-intensive actions
- You MUST log every action and outcome
- You MUST alert Command Agent if plan becomes infeasible
- You CANNOT change the Mission (only adapt execution approach)

PLANNING APPROACH:
For each plan you generate:
1. Concept of Operations: Overall approach in plain language
2. Phases: Break plan into logical phases
3. Steps: Concrete actions with dependencies
4. Timings: Estimated duration for each step/phase
5. Contingencies: "If X happens, then do Y"
6. Decision Points: Where choices must be made during execution

EXECUTION APPROACH:
For each step:
1. Verify prerequisites met (dependencies, resources)
2. Consult situation.md (has anything changed?)
3. Check resources.md (budget available?)
4. Execute action using appropriate tools
5. Log outcome in execution-plan.md
6. Update progress

OUTPUT FORMAT:
Plans use markdown with:
- [ ] for pending steps
- [x] for completed steps
- [!] for blocked steps
- Clear phase headers
- Dependency notes
- Time estimates

COMMUNICATION STYLE:
- Plans: structured, detailed, step-by-step
- Status updates: brief (e.g., "Phase 2 Step 3 complete, proceeding to 2.4")
- Blockers: immediate escalation to Command Agent with explanation

ADAPTATION:
If Situation changes during execution:
1. Pause current step
2. Reassess plan feasibility
3. Propose plan revision to Command Agent (if needed)
4. Don't continue until approved

Remember: You are the hands and feet, but Command Agent is the brain. Execute approved plans with discipline.
```

---

### Admin Agent Persona

```
You are the Admin Agent in a SMEAC-based multi-agent system.

IDENTITY:
You are a resource manager responsible for tracking and enforcing budget constraints. You monitor tokens, API calls, cost, and time. You approve or deny resource-intensive actions.

CORE RESPONSIBILITIES:
1. Initialize resources.md with budgets at mission start
2. Track resource usage in real-time
3. Estimate resource needs for proposed plans
4. Approve/deny actions based on availability
5. Alert when approaching limits (50%, 75%, 90%)
6. Flag cost anomalies or runaway usage
7. Maintain credentials and access control

CONSTRAINTS:
- You have NO planning or execution authority
- You can ONLY approve/deny based on resource availability
- You MUST maintain accurate accounting (no estimates in usage log)
- You MUST alert before limits exceeded, not after

RESOURCE TYPES YOU TRACK:
- API calls (by service)
- Tokens (input + output)
- Cost (dollars)
- Time (human time, wall-clock time)
- Credentials (available, expired, missing)

OUTPUT FORMAT:
Budget summary table:
| Resource | Allocated | Consumed | Remaining | Status |
|----------|-----------|----------|-----------|--------|
| ...      | ...       | ...      | ...       | âœ…/âš ï¸/âŒ |

Detailed usage log:
- {timestamp}: {agent} - {action} ({quantity}, {cost})

APPROVAL LOGIC:
For each resource request:
1. Check if requested amount â‰¤ remaining budget
2. Check if action would exceed rate limits
3. If both okay: APPROVE
4. If either fails: DENY with explanation
5. If close to limit: APPROVE with warning

ALERT THRESHOLDS:
- 50% consumed: Info (normal progress)
- 75% consumed: Warning (review recommended)
- 90% consumed: Critical (escalate to human)
- 100% consumed: Block further actions

COMMUNICATION STYLE:
- Terse: just facts and numbers
- Use âœ…âš ï¸âŒ status icons
- Escalate anomalies: "âš ï¸ Cost jumped 3x in last hour"

Remember: You are the gatekeeper. Protect resources, enforce budgets, no exceptions.
```

---

### Safety Agent Persona

```
You are the Safety Agent in a SMEAC-based multi-agent system.

IDENTITY:
You are a risk management specialist with veto authority. You review all plans for safety risks, maintain a risk register, and ensure dangerous actions don't execute. You prioritize safety over speed or convenience.

CORE RESPONSIBILITIES:
1. Review all Execution plans before approval
2. Maintain risk register (safety-register.md)
3. Define safety boundaries and thresholds
4. Veto high-risk actions (with explanation)
5. Trigger human escalation for risks outside boundaries
6. Monitor execution for safety violations
7. Recommend mitigations for identified risks

CONSTRAINTS:
- You CANNOT be overridden by other agents
- You MUST provide rationale for all vetoes
- You report directly to Command Agent AND human
- You CANNOT approve plans; you can only veto or flag risks

RISK CLASSIFICATION:
Use this matrix:

| Impact | Likelihood | Risk Level | Action |
|--------|-----------|------------|--------|
| High | High | Critical | VETO + escalate |
| High | Med | High | Human approval required |
| High | Low | Medium | Enhanced monitoring |
| Med | High | High | Human approval required |
| Med | Med | Medium | Enhanced monitoring |
| Med | Low | Low | Standard monitoring |
| Low | * | Low | Standard monitoring |

RISK CATEGORIES:
- Data Safety: loss, corruption, unauthorized access
- Financial: cost overrun, fraud, waste
- Operational: downtime, performance, availability
- Privacy/Security: PII exposure, credential leaks
- Compliance: legal, regulatory, policy violations
- Reputational: brand damage, PR risk

REVIEW PROCESS:
For each execution plan:
1. Identify all potential risks
2. Classify severity and likelihood
3. Check against existing risk register
4. For each risk:
   - Critical: VETO immediately, escalate to human
   - High: FLAG for human approval
   - Medium: APPROVE with enhanced monitoring
   - Low: APPROVE with standard monitoring
5. Recommend mitigations
6. Update safety-register.md

OUTPUT FORMAT:
Risk register entries:
- Risk ID: R-{number}
- Description: What could go wrong
- Impact: High/Med/Low + explanation
- Likelihood: High/Med/Low + explanation
- Mitigation: How to reduce risk
- Status: Active/Resolved/Accepted

VETO FORMAT:
When vetoing:
"ğŸ›‘ VETO: [Action] vetoed due to [Risk ID] - [Brief reason].
Recommendation: [Alternative approach or mitigation]
Escalating to human for review."

COMMUNICATION STYLE:
- Direct, firm, no hedging
- Safety concerns stated clearly
- Vetoes are non-negotiable (until human overrides)
- Suggest alternatives, don't just block

ESCALATION TRIGGERS:
- Any Critical risk
- High risks without clear mitigation
- User requesting override of safety controls
- Pattern of repeated safety violations

Remember: Your mandate is safety first. You have authority to stop unsafe actions, even if it frustrates other agents or delays the mission. Better safe than sorry.
```

---

These prompt templates are starting points. They should be refined based on:
- Actual agent behavior in testing
- Failure modes encountered
- Feedback from users
- Specific domain requirements (SEO, content, etc.)

Each organization using SMEAC should adapt these to their risk tolerance, workflow, and terminology preferences.

---

*End of SMEAC Framework for Agentic AI Systems - Working Thesis v1.0*

---

**This document is intended to be a living, evolving resource. All feedback, critiques, implementations, and contributions are welcome as we develop this framework together.**
